{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Mining: A Brief Introduction to Topic Modeling\n",
    "## June 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lineup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Brief review of machine learning definitions</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Case study: JSM Conference Abstracts</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Q/A</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Understand type of machine learning to apply when facing a specific problem</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Familiarize yourself with some algorithms for the types of machine learning described</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Observe Python code and a preferred Data Science development environment</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Inspire creativity in applications for client challenges</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Supervised Machine Learning: data has labels with target output you want to predict</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Unsupervised Machine Learning: data has no labels; algorithm looks for patterns + similarities</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Semi-supervised Machine Learning: some data labels, majority unlabeled</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Reinforcement Learning: algorithm to maximize reward or minimize risk</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is the difference between Machine Learning (ML) & Traditional Statistical Models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not much. They use many of the same models, but with different goals in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ML practitioners' goal is to accurately predict an observation, not necessarily focus on model assumptions & representativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most comprehensive & insightful work will come from a team that includes both machine learning/data science & traditional statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<em>*Additional reading: https://www.kdnuggets.com/2017/08/machine-learning-vs-statistics.html<em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning \"Cheat Sheet\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?export=view&id=1ovhTbem5Mcg09pqILBqZXIYRR-_j9RP7\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "image_ID = '1ovhTbem5Mcg09pqILBqZXIYRR-_j9RP7'\n",
    "Image(url=\"https://drive.google.com/uc?export=view&id={}\".format(image_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview - Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Scrape 10 years of JSM abstracts (2008-2017)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2. Pre-process text data for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3. Conduct topic modeling using 2 approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 4. Review results & limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# JSM website (2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?export=view&id=15vvrVMn5Z8GgghO89msZKjVi1PlN1u3m\" width=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ID = '15vvrVMn5Z8GgghO89msZKjVi1PlN1u3m'\n",
    "Image(url=\"https://drive.google.com/uc?export=view&id={}\".format(image_ID), width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Import initial libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, spacy\n",
    "import en_core_web_sm\n",
    "import nltk, re, os, codecs, mpld3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Suppress certain warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim') # suppress chunksize warning from gensim\n",
    "warnings.filterwarnings(action= 'ignore', category=DeprecationWarning, module= 'scipy') # suppress another scipy warning\n",
    "warnings.filterwarnings(action= 'ignore', category=DeprecationWarning, module= 'pyLDAvis') # suppress another pyLDAvis warning warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import and merge annual JSM data (2008-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataDir = r'..\\data'\n",
    "files = (file for file in os.listdir(dataDir) if re.search(r'.xlsx', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in files:\n",
    "    df = pd.read_excel(r'{}\\{}'.format(dataDir, file_))\n",
    "    df['Year'] = re.search(r'(\\d{4})_.*\\.xlsx', file_).group(1)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<li>remove all observations w/no content in abstract text</li>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>include only observations sponsored by \"survey research methods\"</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### 2. Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>break sentences into individual words (aka tokens)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Lemmatize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>analyzing, analyze, analyzes → analyz*</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### 4. Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>the, of, are, is, thereby, often, nevertheless etc</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>additional stopwords for this data: data, roundtable</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Cleaning\n",
    "Filter all JSM data to include only `Survey Research Methods`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = frame.copy()\n",
    "df = df[~df['Abstract_Text'].isnull()]\n",
    "df['Abstract_Text'] = df['Abstract_Text'].apply(lambda x: re.sub('\\xa0', '', x))\n",
    "df['Abstract_Text'] = df['Abstract_Text'].apply(lambda x: re.sub(r'\\xa0', '', x))\n",
    "df['Abstract_Text'] = df['Abstract_Text'].apply(lambda x: re.sub('^ $', '', x))\n",
    "df = df.loc[df['Abstract_Text'].apply(lambda x: len(x)>0)]\n",
    "df = df[df['Sponsors'].str.contains('Survey Research Methods', na=False)]\n",
    "df = df.drop_duplicates(subset = 'Abstract_Text', keep = 'first')\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38484, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3044, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# II. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Declare what processes you want to apply on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def abstractText(data, process_all= True, lemmatize = False):\n",
    "    nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "    \n",
    "    if process_all and not lemmatize:\n",
    "        return data\n",
    "    \n",
    "    elif process_all and lemmatize:    \n",
    "        return lemmatization(list(sent_to_words(data)), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    elif not process_all and not lemmatize:\n",
    "        return data.apply(lambda x: firstLastSentence(x))\n",
    "    \n",
    "    else:\n",
    "        data = data.apply(lambda x: firstLastSentence(x))\n",
    "        return lemmatization(list(sent_to_words(data)), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Keeps only the first and last sentences of text - assumes those have the most relevant information to abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def firstLastSentence(x):\n",
    "    sentences = x.split('. ')\n",
    "    sentences = [sentence for sentence in sentences if len(sentence)>0]\n",
    "    return '. '.join([sentences[0], sentences[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Functions to prepare for lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    global nlp\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Displaying Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics_and_docs(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print('\\n{}\\n'.format(df['Abstract_Title'].iloc[doc_index])) # show titles associated with topics\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create a document-topic matrix and highlight positive/larger values in green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def display_document_topic_matrix(_model, no_docs, _W):\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(_model.n_components)]\n",
    "    docnames = [\"Doc\" + str(i) for i in range(no_docs)]\n",
    "\n",
    "    df_document_topic = pd.DataFrame(np.round(_W, 2), columns=topicnames, index=docnames)\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "    def color_green(val):\n",
    "        color = 'green' if val > .01 else 'black'\n",
    "        return 'color: {col}'.format(col=color)\n",
    "\n",
    "    def make_bold(val):\n",
    "        weight = 700 if val > .02 else 400\n",
    "        return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "    # Apply Style\n",
    "    df_document_topics_styler = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "    \n",
    "    return df_document_topics_styler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Show top n keywords for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "        \n",
    "        \n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "\n",
    "    return df_topic_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Predicting Topics from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def predict_topic(text, nlp, lemma_flag, model):\n",
    "\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "\n",
    "    # Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = sent_to_words(text)\n",
    "    \n",
    "    # Step 2: Remove stopwords\n",
    "    mytext_3 = [[word for word in item if word not in my_stopwords] for item in sent_to_words(mytext_2)]\n",
    "\n",
    "    # Step 3: Lemmatize or not\n",
    "    if lemma_flag:\n",
    "        # Step 2: Lemmatize\n",
    "        mytext_4 = lemmatization(mytext_3, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    else:\n",
    "        mytext_4 = [' '.join(item) for item in mytext_3]\n",
    "\n",
    "    # Step 3: Vectorize transform\n",
    "    mytext_5 = _vectorizer.transform(mytext_4)\n",
    "\n",
    "    # Step 4: LDA Transform\n",
    "    topic_probability_scores = model.transform(mytext_5)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
    "    return topic, topic_probability_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### Create Topic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def convertLabelsToDict(text, no_topics):\n",
    "    return dict(zip((i for i in range(no_topics)), re.findall(r'Topic \\d+: (.*)', text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "text_20_LDA = \"\"\"\n",
    "Topic 0: Sampling Design\n",
    "Topic 1: Response Issues & Consumer Expenditure Survey\n",
    "Topic 2: Bayesian & Uncertainty Analyses\n",
    "Topic 3: Nonresponse Errors & Adjustments\n",
    "Topic 4: Health Studies\n",
    "Topic 5: Phone Surveys\n",
    "Topic 6: Small Area Estimation\n",
    "Topic 7: Employment Estimates\n",
    "Topic 8: Misc Estimation\n",
    "Topic 9: Disclosure Limitations & Risks\n",
    "Topic 10: UNDETERMINED\n",
    "Topic 11: Linkages\n",
    "Topic 12: Imputation\n",
    "Topic 13: UNDETERMINED\n",
    "Topic 14: Address-Based Sampling\n",
    "Topic 15: UNDETERMINED\n",
    "Topic 16: Data Collection Process\n",
    "Topic 17: Healthcare (Administrative Data)\n",
    "Topic 18: Census & Similar Prgms\n",
    "Topic 19: Misc Analyses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "text_20_NMF = \"\"\"\n",
    "Topic 0: Sampling Design\n",
    "Topic 1: Health Surveys\n",
    "Topic 2: Imputation\n",
    "Topic 3: Small Area Estimation\n",
    "Topic 4: Response Rates\n",
    "Topic 5: Census\n",
    "Topic 6: Phone Surveys\n",
    "Topic 7: Calibration & Weighting\n",
    "Topic 8: American Community Survey (ACS)\n",
    "Topic 9: Variance Estimation\n",
    "Topic 10: Misc Statistics\n",
    "Topic 11: Nonresponse Bias\n",
    "Topic 12: Measurement Error\n",
    "Topic 13: Employment Statistics\n",
    "Topic 14: Address-Based Sampling\n",
    "Topic 15: Interviewer Effects & Behavior\n",
    "Topic 16: Linkages\n",
    "Topic 17: Mode Effects\n",
    "Topic 18: UNDETERMINED\n",
    "Topic 19: Parametric Approaches\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic Modeling (no labeled data) → Unsupervised learning algorithms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Constructing Model Frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Non-negative Matrix Factorization (NMF)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Latent Dirichlet Allocation (LDA)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Both models requires that you assign \"n\" topics in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Unsupervised learning algorithm that extracts useful features</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Represented as a weighted sum of some components</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Product of document-term matrix and topic-term matrix to approximate document-topic matrix</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?export=view&id=1xXNnNXG5CuSeRsDLjah3dEBoro9_v_je\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ID = '1xXNnNXG5CuSeRsDLjah3dEBoro9_v_je'\n",
    "Image(url=\"https://drive.google.com/uc?export=view&id={}\".format(image_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<em>Source: https://en.wikipedia.org/wiki/Non-negative_matrix_factorization<em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def createNMF(min_df, max_df, max_features, stop_words, ngram_range, n_topics, data):\n",
    "    _vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df,\n",
    "                                       max_features = max_features, stop_words= stop_words, ngram_range = ngram_range)\n",
    "    \n",
    "    _vectorized = _vectorizer.fit_transform(data)\n",
    "    _feature_names = _vectorizer.get_feature_names()\n",
    "    \n",
    "    _model = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(_vectorized)\n",
    "    _W = _model.transform(_vectorized)\n",
    "    _H = _model.components_\n",
    "    \n",
    "    return _vectorizer, _vectorized, _feature_names, _model, _W, _H\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Assumes that groups of words that frequently appear together indicate a “topic”</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Assumes that each document contains a mixture of “topics”</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Probabilistic model using P(word | topic) and P(topics | document) – probabilities are calculated iteratively until convergence</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visual of LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?export=view&id=14yNhw5sa740jkkjXbcvGjyoKHvIHkPlE\" width=\"800\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ID = '14yNhw5sa740jkkjXbcvGjyoKHvIHkPlE'\n",
    "Image(url=\"https://drive.google.com/uc?export=view&id={}\".format(image_ID), width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<em>Source: Blei, 2012 https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext<em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def createLDA(min_df, max_df, max_features, stop_words, ngram_range, n_topics, data):\n",
    "    _vectorizer = CountVectorizer(min_df = min_df, max_df = max_df,\n",
    "                                    max_features = max_features,\n",
    "                                    stop_words= stop_words, \n",
    "                                    ngram_range = ngram_range,\n",
    "                                    token_pattern='[a-zA-Z0-9]{3,}')\n",
    "\n",
    "    _vectorized = _vectorizer.fit_transform(data)\n",
    "    _feature_names = _vectorizer.get_feature_names()\n",
    "    \n",
    "    \n",
    "    _model = LatentDirichletAllocation(n_components=n_topics,          # Number of topics\n",
    "                                       max_iter=25,               # Max learning iterations\n",
    "                                       learning_method='batch',  # batch or online - latter is faster on large sets \n",
    "                                       random_state=0,          # Random state\n",
    "#                                       batch_size=124,            # n docs in each learning iter\n",
    "#                                       evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "#                                       learning_decay = 0.5       # set learning rate\n",
    "                                     )\n",
    "    _W = _model.fit_transform(_vectorized)\n",
    "    _H = _model.components_\n",
    "    \n",
    "    return _vectorizer, _vectorized, _feature_names, _model, _W, _H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Process all text? First/Last sentences only?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2. Do you want to lemmatize the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3. Update stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lemma_flag = True\n",
    "process_all_flag = True # False if you want to process first/last sentence only\n",
    "\n",
    "data = abstractText(df['Abstract_Text'], process_all=process_all_flag, lemmatize= lemma_flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_stopwords = ENGLISH_STOP_WORDS.union(['data', 'roundtable', 'datum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# IV. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Definitions of Parameters to show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "no_features = 50000\n",
    "no_topics = 20\n",
    "\n",
    "ngram_range = (1,3)\n",
    "max_df = 0.5\n",
    "min_df = 0.02\n",
    "\n",
    "\n",
    "# for display: \"n\" keywords to show & \"n\" abstract titles\n",
    "no_top_words = 5\n",
    "no_top_documents = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Uncomment whichever model you want to use (NMF or LDA) and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_type = 'NMF'\n",
    "_vectorizer, _vectorized, _feature_names, _model, _W, _H = createNMF(\n",
    "    min_df= min_df, \n",
    "    max_df= max_df, \n",
    "    max_features= no_features,\n",
    "    stop_words= my_stopwords,\n",
    "    ngram_range= ngram_range,\n",
    "    n_topics= no_topics,\n",
    "    data= data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_type = 'LDA'\n",
    "_vectorizer, _vectorized, _feature_names, _model, _W, _H = createLDA(\n",
    "    min_df= min_df, \n",
    "    max_df= max_df, \n",
    "    max_features= no_features,\n",
    "    stop_words= my_stopwords,\n",
    "    ngram_range= ngram_range,\n",
    "    n_topics= no_topics,\n",
    "    data= data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# V. Display Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "saveModel = r'..\\models'\n",
    "saveTextOutput = r'..\\output\\text'\n",
    "saveVizOutput = r'..\\output\\visualizations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_topics = convertLabelsToDict(text_20_NMF, no_topics) # change depending on model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results - NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Displaying topics, top words associated with topic, top titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<em>open text file<em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "sample design sampling size probability\n",
      "\n",
      "Investigating the Performance of Inverse Sampling for Model Estimation\n",
      "\n",
      "\n",
      "Designing Minimum-Cost Multi-Stage Sample Designs\n",
      "\n",
      "\n",
      "Expanding the Number of Primary Sampling Units for the National Health Interview Survey\n",
      "\n",
      "\n",
      "Occupational Requirements Survey Sample Design\n",
      "\n",
      "\n",
      "Adaptive Sampling Using Neyman Allocation\n",
      "\n",
      "\n",
      "Sample Design Research in the 2010 Sample Redesign\n",
      "\n",
      "\n",
      "State and Local Government Sample Design for the National Compensation Survey\n",
      "\n",
      "\n",
      "Studying Millions of Rescued Documents: Sample Plan at the Guatemalan National Police Archive\n",
      "\n",
      "\n",
      "Using Response Rates to Adjust a Dual Sample Design\n",
      "\n",
      "\n",
      "Reducing the Public Employment Survey Sample Size\n",
      "\n",
      "Topic 1:\n",
      "health care national interview medical\n",
      "\n",
      "Using the National Health Interview Survey to Monitor Health Insurance and Access to Care\n",
      "\n",
      "\n",
      "Using the National Health Interview Survey to Monitor the Early Effects of the Affordable Care Act\n",
      "\n",
      "\n",
      "Evaluation of Design Effects for Selected Estimates in the Medical Expenditure Panel Survey\n",
      "\n",
      "\n",
      "Standard Errors: Statistical Consequences of Health Care Provider Insurance Risk Assumption\n",
      "\n",
      "\n",
      "A Comparison of the Health Insurance Coverage Estimates from Four National Surveys and Six State Surveys\n",
      "\n",
      "\n",
      "Health care access data for children and adolescents from the National Survey of Children with Special Health Care Needs and the National Survey of Children's Health\n",
      "\n",
      "\n",
      "Monitoring Health Care Access and Utilization Following Implementation of the Affordable Care Act Using the National Health Interview Survey\n",
      "\n",
      "\n",
      "Methodological Comparison of Estimates of Ambulatory Health Care Use from the Medical Expenditure Panel Survey and Other Data Sources\n",
      "\n",
      "\n",
      "Deriving a Person-Level Weight for Analyzing MEPS Supplemental Data from a Linked Medical Organization Survey\n",
      "\n",
      "\n",
      "Health Care Disparities Research: Using the National Health Care Surveys\n",
      "\n",
      "Topic 2:\n",
      "imputation miss multiple imputation multiple value\n",
      "\n",
      "Hot Deck Imputation: Proper or Improper?\n",
      "\n",
      "\n",
      "Two-phase sampling approach to fractional hot deck imputation\n",
      "\n",
      "\n",
      "An Introduction to Multiple Imputation\n",
      "\n",
      "\n",
      "Comparison of Imputation Techniques for Item Missing Data in the Survey of Income and Program Participation\n",
      "\n",
      "\n",
      "Fractional Imputation for Dummies\n",
      "\n",
      "\n",
      "Extension of Fractional Imputation to General Missingness Patterns using Maximum Likelihood\n",
      "\n",
      "\n",
      "Imputing with Confidence\n",
      "\n",
      "\n",
      "Recent Developments in Fractional Imputation\n",
      "\n",
      "\n",
      "Diagnosing Imputation Models by Applying Target Analyses Under Posterior Replications of Observed Data\n",
      "\n",
      "\n",
      "Restricted Latent Class Multiple Imputation Method of Categorical Missing Data\n",
      "\n",
      "Topic 3:\n",
      "area small small area area estimation small area estimation\n",
      "\n",
      "Current Developments in Small-Area Estimation at Statistics Canada\n",
      "\n",
      "\n",
      "Inference About Small-Area Distributions Using Area-Level Tabulations\n",
      "\n",
      "\n",
      "Small Area Estimation under Density Ratio Model\n",
      "\n",
      "\n",
      "Nonparametric Small-Area Estimation\n",
      "\n",
      "\n",
      "An Objective Stepwise Bayes Approach to Small Area Estimation \n",
      "\n",
      "\n",
      "Small-Area Estimation Under Measurement Error Models\n",
      "\n",
      "\n",
      "On Estimating Mean Squared Prediction Error of Small Area Estimators in Basic Area Level Model with Unknown Sampling Variance by Parametric Bootstrap\n",
      "\n",
      "\n",
      "An Empirical Study to Evaluate the Performance of Synthetic Estimates of Substance Use in the National Survey on Drug Use and Health\n",
      "\n",
      "\n",
      "Design-Based Evaluation of Bayesian Methods for Complex Survey Data\n",
      "\n",
      "\n",
      "A Hierarchical Mixture Model for Small-Area Estimation\n",
      "\n",
      "Topic 4:\n",
      "response rate response rate respondent non\n",
      "\n",
      "Response Rates Revisited\n",
      "\n",
      "\n",
      "Follow-Up Survey Response Rates in Women at Risk for Breast Cancer\n",
      "\n",
      "\n",
      "An Investigation on Response Rate for Best Survey Estimates of Inflation Expectations\n",
      "\n",
      "\n",
      "The Impact on Response Rates of Adding a Survey Supplement\n",
      "\n",
      "\n",
      "Will They Answer the Phone if They Know It Is Us? Using Caller ID to Improve Response Rates\n",
      "\n",
      "\n",
      "A Randomized Experiment to Increase Response Rates to a Health Care Survey Among Individuals with a High Predicted Probability of Preferring Spanish\n",
      "\n",
      "\n",
      "Survey Quality Indicator Measures: Response Rates and Alternatives\n",
      "\n",
      "\n",
      "Strategies for Subsampling Nonrespondents for Economic Programs\n",
      "\n",
      "\n",
      "Extending Hansen and Hurwitz's Approach for Nonresponse in Sample Survey\n",
      "\n",
      "\n",
      "Modeling Response Rates for Telephone Surveys\n",
      "\n",
      "Topic 5:\n",
      "census bureau census bureau coverage operation\n",
      "\n",
      "Examining Components of Coverage for the 2010 U.S. Census for Several Census Operations\n",
      "\n",
      "\n",
      "The Equivalence of Neyman Optimum Allocation for Sampling and Equal Proportions for Apportioning the U. S. House of Representatives\n",
      "\n",
      "\n",
      "Using 2010 Census Coverage Measurement Results to Better Understand Possible Administrative Records Incorporation in the Decennial Census\n",
      "\n",
      "\n",
      "The 2011 UK Census and Its Coverage\n",
      "\n",
      "\n",
      "United States Census Coverage Survey Results\n",
      "\n",
      "\n",
      "Impact of Outreach Initiatives on the 2007 Census of Agriculture\n",
      "\n",
      "\n",
      "Characteristics of the 2010 Census Nonresponse Follow-Up Operation\n",
      "\n",
      "\n",
      "'You Really Have to Puzzle This Out': Checking Residence and Coverage Duplications on a Census 2010 Questionnaire\n",
      "\n",
      "\n",
      "Missing Data Methods for CCM Component Error Estimation\n",
      "\n",
      "\n",
      "Recursive Partitioning for Racial Classification Cells\n",
      "\n",
      "Topic 6:\n",
      "cell phone cell phone landline rdd\n",
      "\n",
      "Dual-Frame Weights (Landline and Cell) for the 2009 Minnesota Health Access Survey\n",
      "\n",
      "\n",
      "Cell Phone--Only Household in a National Mail Survey: Who Are They?\n",
      "\n",
      "\n",
      "Cell Phone Versus Landline Respondents: Who's More Cooperative After Screening?\n",
      "\n",
      "\n",
      "Estimating the Bias Resulting from the Exclusion of Cell Phone-Only Respondents\n",
      "\n",
      "\n",
      "Noncoverage Bias in Household Landline Telephone Surveys: The BRFSS Experience\n",
      "\n",
      "\n",
      "Dual Frame RDD Survey Costs: Landline vs. Cell Phone Comparisons\n",
      "\n",
      "\n",
      "Dual-Frame Sample Sizes (RDD and Cell) for Future Minnesota Health Access Surveys\n",
      "\n",
      "\n",
      "Finding an Optimal Composite Factor: Weighting Data from a Household Survey Using a Cell Overlap Design\n",
      "\n",
      "\n",
      "Propensity Score Matching to Correct Telephone Surveys for Cell Phone Nonresponse\n",
      "\n",
      "\n",
      "Evaluating the Effects of Adding Cell Phone Samples to the Traditional Landline Phone Samples on Prevalence Estimates from a Telephone Call-Back Survey \n",
      "\n",
      "Topic 7:\n",
      "weight calibration adjustment weighting survey weight\n",
      "\n",
      "Misspecified Sampling Weights in Weight-Smoothing Methods\n",
      "\n",
      "\n",
      "Comparing Alternative Weight Adjustment Methods \n",
      "\n",
      "\n",
      "Simultaneous Calibration and Nonresponse Adjustment\n",
      "\n",
      "\n",
      "Improving Precision by Calculating Estimates During the Calibration Process\n",
      "\n",
      "\n",
      "Single-Stage Generalized Raking Weight Adjustment in the Current Population Survey\n",
      "\n",
      "\n",
      "Empirical Evaluation of Raking Ratio Adjustments for Nonresponse\n",
      "\n",
      "\n",
      "Bayesian Modeling for Weighting Adjustment and Inference in Sample Surveys\n",
      "\n",
      "\n",
      "Practical Issues When Calibrating Weights for Multiple Skewed Variables\n",
      "\n",
      "\n",
      "Power-Shrinkage: An Alternative Method for Dealing with Excessive Weights\n",
      "\n",
      "\n",
      "Multilevel Regression and Post-Stratification for Survey Weighting\n",
      "\n",
      "Topic 8:\n",
      "estimate acs state county year\n",
      "\n",
      "Developing Guidelines Based on CVs for When One-Year Estimates Can Be Used Instead of Three-Year Estimates in the American Community Survey\n",
      "\n",
      "\n",
      "Unit-Level Logistic Mixed Effects Models for Small Area Estimation of Poverty Estimates\n",
      "\n",
      "\n",
      "Improving Small-Area Estimates of Disability: A Model-Based Approach to Combining the American Community Survey with the Survey of Income and Program Participation\n",
      "\n",
      "\n",
      "Usability of the American Community Survey Estimates of the Group Quarters Population for Substate Geographies\n",
      "\n",
      "\n",
      "American Community Survey Sample Size Research\n",
      "\n",
      "\n",
      "Using Sub-County Population Estimates as Controls in Weighting for the American Community Survey\n",
      "\n",
      "\n",
      "Assessment of Data Release Rules on the Reliability of Multi-Year Estimates in American Community Survey Data Products\n",
      "\n",
      "\n",
      "Evaluation of the Effectiveness of the American Community Survey Family Equalization Project\n",
      "\n",
      "\n",
      "Using Imputation Methods to Improve the American Community Survey Estimates of the Group Quarters Population for Small Geographies\n",
      "\n",
      "\n",
      "Applying Bivariate Binomial/Logit Normal Models to Small-Area Estimation\n",
      "\n",
      "Topic 9:\n",
      "estimator variance variance estimator mean estimation\n",
      "\n",
      "Variance Estimation for a Small Number of PSUs\n",
      "\n",
      "\n",
      "Cross-Validation in Survey Estimation: Model Selection and Variance Estimation\n",
      "\n",
      "\n",
      "Modified Ratio Estimators in Simple Random Sampling\n",
      "\n",
      "\n",
      "Estimation of Finite Population Variance Using Scrambled Responses in the Presence of Auxiliary Information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A Class of Dual Frame Survey Sampling Estimators in the Presence of a Covariate: How Amy Predicts Her President\n",
      "\n",
      "\n",
      "Propensity Score Adjustments Using Covariates in Observational Studies\n",
      "\n",
      "\n",
      "Analysis on Generalized Variance Function Estimators from Complex Sample Surveys\n",
      "\n",
      "\n",
      "A New Optimal Estimator of Population Proportion in Randomized Response Sampling\n",
      "\n",
      "\n",
      "Using Successive Difference Replication for Estimating Variances\n",
      "\n",
      "\n",
      "Small Area Estimation Under Fay-Herriot Models with Nonparametric Estimation of Error Variances\n",
      "\n",
      "Topic 10:\n",
      "statistical analysis research risk time\n",
      "\n",
      "Confidentiality Approaches for Real-Time Systems Generating Aggregated Results\n",
      "\n",
      "\n",
      "nan\n",
      "\n",
      "\n",
      "Revising Statistical Standards to Keep Pace with the Web\n",
      "\n",
      "\n",
      "Human Rights and Statistics: A Reciprocal Relationship\n",
      "\n",
      "\n",
      "Managing Disclosure Risks in the Curation and Dissemination of Research Data\n",
      "\n",
      "\n",
      "Comparative Study of Differentially Private Data Synthesis Methods\n",
      "\n",
      "\n",
      "Rethinking the Risk-Utility Tradeoff Approach to Statistical Disclosure Limitation\n",
      "\n",
      "\n",
      "Utilizing Automated Statistical Edit Changes in Significance Editing\n",
      "\n",
      "\n",
      "Innovative Guidance for the Design of New Surveys or Major Revisions of Existing Surveys\n",
      "\n",
      "\n",
      "Releasing Microdata: Disclosure Risk Estimation, Data Masking, and Assessing Utility\n",
      "\n",
      "Topic 11:\n",
      "nonresponse bias nonresponse bias adjustment propensity\n",
      "\n",
      "Weighting Methods in Survey Sampling\n",
      "\n",
      "\n",
      "A Model-Based Approach to Assessing and Mitigating Nonresponse Bias for the Monthly Wholesale Trade Survey\n",
      "\n",
      "\n",
      "Analyzing Survey Nonresponse\n",
      "\n",
      "\n",
      "Representativeness (R-Index) and Nonresponse Bias Patterns in Household Surveys\n",
      "\n",
      "\n",
      "Using Call-Back Data to Adjust for Nonignorable Nonresponse: Results of an Empirical Study\n",
      "\n",
      "\n",
      "Assessment and Evaluation of Nonresponse Error in the Medical Expenditure Panel Survey\n",
      "\n",
      "\n",
      "Effects of Link Misspecification in Propensity Score Weighting Adjustment\n",
      "\n",
      "\n",
      "Assessing the Effect of Calibration on Nonresponse Bias in the 2005 Agricultural Resource Management Survey Phase III Sample Using Census 2002 Data\n",
      "\n",
      "\n",
      "Nonresponse Bias Analysis and Sample Selection Evaluation in the Survey of Business Owners\n",
      "\n",
      "\n",
      "Identifying, Collecting, and Using Auxiliary Variables To Adjust for Nonresponse Bias in Organizational Surveys\n",
      "\n",
      "Topic 12:\n",
      "error measurement measurement error estimate source\n",
      "\n",
      "Evaluating the Quality of Survey and Administrative Data with Generalized Multitrait-Multimethod Models\n",
      "\n",
      "\n",
      "Measurement Error in Small Area Estimation: Functional vs. Structural vs. Naive Models\n",
      "\n",
      "\n",
      "A Measurement Error Model for Self-Reported Physical Activity\n",
      "\n",
      "\n",
      "Assessing the Impact of Measurement Error in Modeling Change: A Sensitivity Analysis Approach\n",
      "\n",
      "\n",
      "An Investigation of Interviewer Effects on Measurement Error\n",
      "\n",
      "\n",
      "Data Fusion for Correcting Measurement Errors\n",
      "\n",
      "\n",
      "Addressing Differential Measurement Error in Self-Reported Dietary Data Using an External Validation Study: Application to a Longitudinal Lifestyle Intervention Trial\n",
      "\n",
      "\n",
      "Response Propensity and Motivated Under-Reporting: Do Persons Likely to Respond Give Better Answers to Filter and Eligibility Questions?\n",
      "\n",
      "\n",
      "Small-Area Estimation in the Presence of Measurement Error Using Corrected Scores Approach\n",
      "\n",
      "\n",
      "Methods for Fitting a Markov Latent Class Analysis for the National Crime Victimization Survey\n",
      "\n",
      "Topic 13:\n",
      "employment establishment statistic labor labor statistic\n",
      "\n",
      "Response Analysis Survey: Examining Reasons for Employment Differences Between the Quarterly Census of Employment and Wages and the Current Employment Statistics Survey\n",
      "\n",
      "\n",
      "Evaluation of Alternative Measures of Size for Sampling of Establishments in the National Compensation Survey\n",
      "\n",
      "\n",
      "Reconciling Employment Differences Between Administrative and Survey Data\n",
      "\n",
      "\n",
      "Improving Estimates of Employment in Expanding and Contracting Businesses\n",
      "\n",
      "\n",
      "Bias Analysis of Average Weekly Earnings in the Current Employment Statistics Survey\n",
      "\n",
      "\n",
      "Adjusting for Nonresponse in the Occupational Employment Statistics Survey\n",
      "\n",
      "\n",
      "Integrating Sample Designs for Environmental Industry and Occupational Employment Surveys\n",
      "\n",
      "\n",
      "Research on Quarterly Benchmarking for the Current Employment Statistics Survey\n",
      "\n",
      "\n",
      "An Evaluation Of Bls Noise Research For The Quarterly Census Of Employment And Wages\n",
      "\n",
      "\n",
      "Development of JOLTS Firm Size Estimation Methodology\n",
      "\n",
      "Topic 14:\n",
      "frame address list household coverage\n",
      "\n",
      "A Comparative Evaluation of Traditional Listing vs. Address-Based Sampling Frames: Matching with Field Investigation of Discrepancies\n",
      "\n",
      "\n",
      "A National Evaluation of Coverage for a Sampling Frame Based on the Master Address File\n",
      "\n",
      "\n",
      "Experiences with the Use of Address-Based Sampling in Two In-Person National Household Surveys\n",
      "\n",
      "\n",
      "Address-Based Sampling Frames for Beginners\n",
      "\n",
      "\n",
      "Handling Frame Problems with Address-Based Sampling and In-Person Household Surveys\n",
      "\n",
      "\n",
      "Is Housing Unit Undercoverage Random?\n",
      "\n",
      "\n",
      "Assessing the Filter Rules for Extracting Addresses from the Master Address File To Construct a Housing Unit Frame for Current Demographic Surveys\n",
      "\n",
      "\n",
      "A Comparison of Address-Based Sampling and Random Digit Dialing Sampling\n",
      "\n",
      "\n",
      "Adjusting an Area Frame Estimate for Misclassification Using a List Frame\n",
      "\n",
      "\n",
      "ABS Coverage Evaluation: Recommendations for Evaluating the Household Coverage of Address-Based Sampling (ABS) Frames\n",
      "\n",
      "Topic 15:\n",
      "interviewer respondent interview question case\n",
      "\n",
      "Interviewer Effects and the Measurement of Financial Literacy\n",
      "\n",
      "\n",
      "Interviewer Behavior and Survey Data Quality: The Case of Social Network Data\n",
      "\n",
      "\n",
      "How Much of Interviewer Variance Is Really Nonresponse Error Variance?\n",
      "\n",
      "\n",
      "Where Do Interviewers Go When They Do What They Do? An Analysis of Interviewer Travel in Two Field Surveys\n",
      "\n",
      "\n",
      "Interviewer Effects on Racially-Sensitive Questions and Biomeasures: Results from a Semi-Interpenetrated Design\n",
      "\n",
      "\n",
      "What's the Chance? Interviewers' Expectations of Response in the 2010 SCF\n",
      "\n",
      "\n",
      "Modeling the Difference in Interview Characteristics for Different Respondents\n",
      "\n",
      "\n",
      "Estimating Interviewer Effects in the Absence of Interpenetration\n",
      "\n",
      "\n",
      "Do Interviewers with High Cooperation Rates Behave Differently? Interviewer Cooperation Rates and Interview Behaviors\n",
      "\n",
      "\n",
      "Investigating the Effect of Interviewer Job Attitudes on Turnover and Job Performance in Centralized Telephone Interviewing Facilities\n",
      "\n",
      "Topic 16:\n",
      "record linkage link administrative administrative record\n",
      "\n",
      "A Cross-Disciplinary Review of Record Linkage Methodologies\n",
      "\n",
      "\n",
      "Transitive Probabilistic Deduplication of Record Systems Using a Stochastic Blockmodel\n",
      "\n",
      "\n",
      "Evaluating Record Linkage Quality in the NCHS Linked Mortality Files\n",
      "\n",
      "\n",
      "The Limit of Linkage: What Happens When Records Lack PII\n",
      "\n",
      "\n",
      "Trends in Record Linkage Refusal Rates: Characteristics of National Health Interview Survey Participants Who Refuse Record Linkage\n",
      "\n",
      "\n",
      "Methods for Linking Administrative Records When Unique Identifiers Are Not Available: Using Project Talent and the Death Master File\n",
      "\n",
      "\n",
      "A Simplified Approach to Administrative Record Linkage in the Quarterly Census of Employment and Wages\n",
      "\n",
      "\n",
      "Modeling Similar Nonmatches in Record Linkage with Mixture Models \n",
      "\n",
      "\n",
      "Using Statistical Matching as a Supplement to Exact Record Linkage\n",
      "\n",
      "\n",
      "A Bayesian Approach to Incorporating Uncertainty in Record Linkage \n",
      "\n",
      "Topic 17:\n",
      "mode web effect mail telephone\n",
      "\n",
      "Mode Effects and Data Quality on a Survey of New Businesses\n",
      "\n",
      "\n",
      "Adjusting for Effects of Survey Mode Difference Across a Longitudinal Mixed-Mode Study\n",
      "\n",
      "\n",
      "Investigating the Bias of Alternative Statistical Inference Methods in Mixed-Mode Surveys\n",
      "\n",
      "\n",
      "An Analysis of Mode Effects in Three Mixed-Mode Surveys of Veteran and Military Populations\n",
      "\n",
      "\n",
      "Survey Mode Effects in Two Military Surveys\n",
      "\n",
      "\n",
      "Framing Inference from Mixed-Mode Surveys Using Causal Inference Framework\n",
      "\n",
      "\n",
      "Adaptive Mixed-Mode Survey Designs Accounting for Mode Effects\n",
      "\n",
      "\n",
      "Using an Item Response Theory Approach to Measure Survey Mode of Administration Effects: Analysis of Data from a Randomized Mode Experiment\n",
      "\n",
      "\n",
      "Investigating the Bias of Alternative Statistical Inference Methods in Sequential Mixed-Mode Surveys\n",
      "\n",
      "\n",
      "Mode Effects in American Trends Panel: A Closer Look at the Person-Level and Item-Level Characteristics\n",
      "\n",
      "Topic 18:\n",
      "method propose study simulation distribution\n",
      "\n",
      "Empirical Likelihood-Based Method Using Calibration for Longitudinal Data with Drop-Out\n",
      "\n",
      "\n",
      "An Integrated Adaptive Approach to Data Fusion\n",
      "\n",
      "\n",
      "Effects of Missing and Censored Data for Nonlinear Models Involving ODEs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Secondary Analysis in GWAS\n",
      "\n",
      "\n",
      "Cancer Marker Identification via Penalized Integrative Analysis\n",
      "\n",
      "\n",
      "Confidence Intervals for the Ratio of Two Poisson Rates\n",
      "\n",
      "\n",
      "Propensity Score Adjustment Method for Nonignorable Nonresponse\n",
      "\n",
      "\n",
      "Modeling of Longitudinal Biomarker Data with Dropout and Death Using a Weighted Pseudo--Maximum Likelihood Method\n",
      "\n",
      "\n",
      "Haplotype-Based Association Studies Under Complex Sampling\n",
      "\n",
      "\n",
      "Bayesian Finite Population Inference for Skewed Survey Data Using Skew-Normal Penalized-Spline Regression\n",
      "\n",
      "Topic 19:\n",
      "model effect regression linear variable\n",
      "\n",
      "A semi-parametric approach to fractional imputation for nonignorable missing data\n",
      "\n",
      "\n",
      "A Semiparametric Approach to Inference with Nonignorable Missing Data Using Surrogate Information\n",
      "\n",
      "\n",
      "Nonparametric and Semiparametric M-Quantile Inference for Longitudinal Data\n",
      "\n",
      "\n",
      "Application of GEE and MRM in Evaluation of the Efficacy of an HIV Prevention Intervention\n",
      "\n",
      "\n",
      "A Picture Is Worth a Billion Words: Visualizing Mega-Parameter Models from Giga-Scale Textual Data\n",
      "\n",
      "\n",
      "Joint Modeling of Longitudinal and Survival Data with Missing and Left-Censored Time-Varying Covariates\n",
      "\n",
      "\n",
      "Evaluating the Uncertainty in HIV/AIDS Infection Models\n",
      "\n",
      "\n",
      "On Modeling and Estimation of Response Probabilities When Missing Data Are Not Missing at Random\n",
      "\n",
      "\n",
      "Model-Calibration Estimator Using Time-To-Event Models\n",
      "\n",
      "\n",
      "Robust Bayesian Small-Area Estimation for Area-Level Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_topics_and_docs(_H, _W, _feature_names, data, no_top_words, no_top_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Write the above display of topics and relevant titles to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'{}\\{}_Topics_and_top{}_abstract_titles.txt'.format(saveTextOutput, model_type, no_top_documents), 'w') as f:\n",
    "    \n",
    "    for topic_idx, topic in enumerate(_H):\n",
    "        f.write(\"Topic {}:\\n\\n\".format(topic_idx))\n",
    "        f.write(\" \".join([_feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        f.write('\\n')\n",
    "\n",
    "        top_doc_indices = np.argsort(_W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for num, doc_index in enumerate(top_doc_indices):\n",
    "            f.write('\\n{}. {}\\n'.format(num, df['Abstract_Title'].iloc[doc_index])) # show titles associated with topics\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### View document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "doc_topic_matrix = display_document_topic_matrix(_model=_model, no_docs=len(data), _W = _W)\n",
    "# doc_topic_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### View Top 5 keywords for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>sample</td>\n",
       "      <td>design</td>\n",
       "      <td>sampling</td>\n",
       "      <td>size</td>\n",
       "      <td>probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>health</td>\n",
       "      <td>care</td>\n",
       "      <td>national</td>\n",
       "      <td>interview</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>imputation</td>\n",
       "      <td>miss</td>\n",
       "      <td>multiple imputation</td>\n",
       "      <td>multiple</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>area</td>\n",
       "      <td>small</td>\n",
       "      <td>small area</td>\n",
       "      <td>area estimation</td>\n",
       "      <td>small area estimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>response</td>\n",
       "      <td>rate</td>\n",
       "      <td>response rate</td>\n",
       "      <td>respondent</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word 0  Word 1               Word 2           Word 3  \\\n",
       "Topic 0      sample  design             sampling             size   \n",
       "Topic 1      health    care             national        interview   \n",
       "Topic 2  imputation    miss  multiple imputation         multiple   \n",
       "Topic 3        area   small           small area  area estimation   \n",
       "Topic 4    response    rate        response rate       respondent   \n",
       "\n",
       "                        Word 4  \n",
       "Topic 0            probability  \n",
       "Topic 1                medical  \n",
       "Topic 2                  value  \n",
       "Topic 3  small area estimation  \n",
       "Topic 4                    non  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_keywords = show_topics(vectorizer=_vectorizer, model=_model, n_words=5)        \n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predict Topic(s) Based on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2018 Abstracts in Survey Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=329351</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=330627</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste text here: Current Population Survey (CPS) is the oldest survey in the United States since 1942 and the source of numerous high-profile economic statistics, including the national unemployment rate. Balanced repeated replication (BRR) is the main methodology used for variance estimation in CPS as well as many other surveys conducted by the U.S. Census Bureau. In this talk, we study the properties of collapsed-stratum BRR implemented at the Census Bureau in estimating variance of CPS household response rate in non-self-representing (NSR) strata. In addition, we will present some bias study in BRR variance estimate using simulations based on CPS data as a frame.\n"
     ]
    }
   ],
   "source": [
    "mytext = [input('Paste text here: ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most dominant topic(s): \n",
      "Topic: 5\n",
      "Census,\n",
      "\n",
      "Topic: 4\n",
      "Response Rates,\n",
      "\n",
      "Topic: 8\n",
      "American Community Survey (ACS),\n",
      "\n",
      "Topic: 9\n",
      "Variance Estimation,\n",
      "\n",
      "Topic: 13\n",
      "Employment Statistics\n"
     ]
    }
   ],
   "source": [
    "topic, prob_scores = predict_topic(text =mytext, nlp=nlp, lemma_flag= lemma_flag, model = _model)\n",
    "\n",
    "# run this if you don't have labels for your topics\n",
    "# print(r'Most dominant topic(s): {}'.format(', '.join([df_topic_keywords.index[i] for i in np.argsort(-prob_scores)[0,:3] \n",
    "#                                                       if prob_scores[0, i]> 0.01])))\n",
    "\n",
    "# # run this if you do have labels for your topics\n",
    "print('Most dominant topic(s): \\n{}'.format(',\\n\\n'.join(['\\n'.join(('Topic: ' + str(i), _topics[i])) \n",
    "                                                        for i in np.argsort(-prob_scores)[0,:5] if prob_scores[0, i]> 0.01])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# VII. Visualizing Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Import viz libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Initialize t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# angle value close to 1 means sacrificing accuracy for speed\n",
    "# pca initialization usually leads to better results \n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "\n",
    "\n",
    "tsne_= tsne_model.fit_transform(_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cust_color_palette = np.array([\"#e6194b\", \"#3cb44b\", \"#ffe119\", \"#0082c8\", \"#f58231\",\n",
    "                               \"#911eb4\", \"#46f0f0\", \"#f032e6\", \"#d2f53c\", \"#fabebe\",\n",
    "                               \"#008080\", \"#e6beff\", \"#aa6e28\", \"#fffac8\", \"#800000\",\n",
    "                               \"#aaffc3\", \"#808000\", \"#ffd8b1\", \"#000080\", \"#808080\",\n",
    "                              \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\",\"#2ca02c\"])\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_keys = []\n",
    "for i in range(_W.shape[0]):\n",
    "    _keys +=  _W[i].argmax(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_)\n",
    "df_tsne.rename(columns = {0: 'X_tsne', 1: 'Y_tsne'}, inplace = True)\n",
    "df_topics = pd.DataFrame(_W)\n",
    "df_topics['ind'] = _keys\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.merge(df_topics, df_tsne, how = 'inner', left_index=True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook, save, output_file\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "source = bp.ColumnDataSource(dict(\n",
    "    x=df2['X_tsne'],\n",
    "    y=df2['Y_tsne'],\n",
    "    color=cust_color_palette[_keys],\n",
    "#     topic_key= df2['ind'],\n",
    "    topic_key= df2['ind'].apply(lambda l: _topics[l]), # dictionary here needs to be updated as you change models\n",
    "    title= df['Abstract_Title'],\n",
    "    content = df['Abstract_Text']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing results - scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "title = 'Visualization of {} topics ({})'.format(no_topics, model_type)\n",
    "\n",
    "_model = figure(plot_width=1100, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "_model.scatter(x='x', y='y', legend='topic_key', source=source,\n",
    "                 color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "# hover tools\n",
    "hover = _model.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"Title\": \"@title\", \"Topic\": \"@topic_key\"} #, KeyWords: @content - Topic: @topic_key \"}\n",
    "_model.legend.location = \"top_left\"\n",
    "\n",
    "# move legend to outside of chart area\n",
    "new_legend = _model.legend[0]\n",
    "_model.legend[0].plot = None\n",
    "_model.add_layout(new_legend, 'right')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show(_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "filename = r'{}_scatterplot_{}_topics.html'.format(model_type, no_topics)\n",
    "\n",
    "\n",
    "output_file(r'{}\\{}'.format(saveVizOutput, filename))\n",
    "save(_model)\n",
    "\n",
    "# bp.reset_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# VIII. Visualizing LDA (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Have to re-run LDA script here because the variables stored after running function above do not retain arributes that are needed to plug into the viz method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_vectorizer = CountVectorizer(min_df = min_df, max_df = max_df,\n",
    "                                    max_features = no_features,\n",
    "                                    stop_words= my_stopwords, \n",
    "                                    ngram_range = ngram_range,\n",
    "                                    token_pattern='[a-zA-Z0-9]{3,}')\n",
    "\n",
    "_vectorized = _vectorizer.fit_transform(data)\n",
    "_feature_names = _vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "_model = LatentDirichletAllocation(n_components=no_topics,          # Number of topics\n",
    "                                   max_iter=25,               # Max learning iterations\n",
    "                                   learning_method='batch',  # batch or online - latter is faster on large sets \n",
    "                                   random_state=0,          # Random state\n",
    "#                                       batch_size=124,            # n docs in each learning iter\n",
    "#                                       evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                  n_jobs = -1,               # Use all available CPUs\n",
    "#                                       learning_decay = 0.5       # set learning rate\n",
    "                                 )\n",
    "_W = _model.fit_transform(_vectorized)\n",
    "_H = _model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing results - intertopic distance (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model= _model, dtm=_vectorized, vectorizer=_vectorizer, mds='pca')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(panel, '{}\\intertopic_distance_LDA_{}topics.html'.format(saveVizOutput, no_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Results from any unsupervised machine learning model should be taken with a grain of salt</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>“Topics” as uncovered by machine learning models may not be the same as how humans understand a coherent topic; for machine learning models, “topics” are comprised of components which may or may not have semantic meaning</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "><li>Two author writing about different subjects could be seen as having similar “topics” because of their writing style and preferred word choices</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Pre-processing data and model parameters may dramatically change the separation of topics</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?\n",
    "## email: Alison_Thaung@abtassoc.com\n",
    "## twitter: @AlisonThaung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import Needed Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium: accesses and controls web browser to submit search and locate web elements by xpath\n",
    "\n",
    "CSV: reads and writes CSV files\n",
    "\n",
    "re: regular expression for pattern matching\n",
    "\n",
    "requests: makes HTTP requests\n",
    "\n",
    "BeautifulSoup:parses HTML\n",
    "\n",
    "pandas: data analysis library to manage dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import csv, re, requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Instantiate Browser Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ww2.amstat.org/meetings/jsm/2008/onlineprogram/'\n",
    "\n",
    "# chromedriver needs to be in exectuable path or you need to map to where your chromedriver is\n",
    "\n",
    "# move up to levels to where chromedriver is saved\n",
    "chrome_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "browser = webdriver.Chrome(executable_path= os.path.join(chrome_path, 'chromedriver'))\n",
    "browser.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Navigate through webpage and scrape objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat xpaths for needed items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_search = '/html/body/table[2]/tbody/tr[2]/td/form/div/input[6]'\n",
    "xpath_session = '//tr//td[1]//a[contains(@href, \"activity_details\")]'\n",
    "xpath_titles = '//tr//td[2]'\n",
    "xpath_sponsors = '//tbody//tr//td[3]'\n",
    "xpath_type = '//tbody//tr/td[4]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click search with empty fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wouldn't click on this page so had to execute script instead of standard click() method\n",
    "element = browser.find_element_by_xpath(xpath_search)\n",
    "browser.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "\n",
    "# browser.find_element_by_xpath(xpath_search).click()\n",
    "# wait = WebDriverWait(browser, 30)\n",
    "# wait.until(EC.presence_of_element_located((By.XPATH, xpath_search)))\n",
    "# browser.find_element_by_xpath(xpath_search).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of each field needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [item.text for item in browser.find_elements_by_xpath(xpath_session) if 'Session' not in item.text]\n",
    "titles = [item.text for item in browser.find_elements_by_xpath(xpath_titles) if item.text != \"Title\"]\n",
    "sponsors = [item.text.replace('\\n', ', ') for item in browser.find_elements_by_xpath(xpath_sponsors) \n",
    "            if item.text != 'Sponsor']\n",
    "session_type = [re.sub(r'(\\n|\\r|\\t)', ' ', item.text) for item in browser.find_elements_by_xpath(xpath_type) \n",
    "                if item.text != \"Type\" ]\n",
    "session_linkToAbstract = [item.get_attribute('href') for item in browser.find_elements_by_xpath(xpath_session)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sessions) == len(titles) == len(sponsors) == len(session_linkToAbstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Session_No' : sessions,\n",
    "                   'Session_Title' : titles,\n",
    "                   'Sponsors': sponsors,\n",
    "                   'Type': session_type,\n",
    "                   'Session_URL': session_linkToAbstract                   \n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Session_ID'] = df['Session_URL'].apply(lambda x: re.sub(r'https\\S+((\\d){6})', r'\\1', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = 'Session_URL', inplace = True)\n",
    "df.reset_index(inplace = True, drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = r'C:\\Users\\Thaunga\\Scripts\\02. Presentations\\JSM\\Datasets'\n",
    "df.to_excel(r'{}\\2008Sessions.xlsx'.format(dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code for this year's object locations (xpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Session_No'].apply(lambda x: bool(re.search(r'(?<!\\d)\\d{1,3}(?!\\d)',x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df['Session_URL'].iloc[45]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(test)\n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item.get('href') for item in soup.find_all('a') if re.search('abstract_details', str(item.get('href')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code to extract workshops and abstract_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of abstracts to scrape with new urls\n",
    "session_url = []\n",
    "abstract_url = []\n",
    "abstract_title = []\n",
    "session_no_v2 = []\n",
    "\n",
    "#dataframe of abstracts already written in the session\n",
    "workshop_abstract = []\n",
    "workshop_session_no = []\n",
    "workshop_session_url = []\n",
    "\n",
    "for link in session_linkToAbstract:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    # extract abstract titles and links\n",
    "    abstract_links = [item.get('href') for item in soup.find_all('a') if re.search('abstract_details', str(item.get('href')))]\n",
    "    \n",
    "    if len(abstract_links) == 0:\n",
    "        try:\n",
    "            abstract_workshop = re.sub(r'(\\n|\\r|\\t)', '', soup.find_all('tr')[9].text)\n",
    "            id_no = soup.find_all('tr')[5].find('strong').text.strip()\n",
    "\n",
    "            workshop_abstract.append(abstract_workshop)\n",
    "            workshop_session_no.append(id_no)\n",
    "            workshop_session_url.append(link)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        abstract_titles = [item.text for item in soup.find_all('a') \n",
    "            if re.search('abstract_details', str(item.get('href')))]\n",
    "\n",
    "        # map back to session id and original link for later joining\n",
    "        session_ids = [soup.find_all('tr')[5].find('strong').text] * len(abstract_links)\n",
    "        original_link = [link] * len(abstract_links)\n",
    "\n",
    "        # create list of \n",
    "        session_url = session_url + original_link\n",
    "        abstract_url = abstract_url + abstract_links\n",
    "        abstract_title = abstract_title + abstract_titles\n",
    "        session_no_v2 = session_no_v2 + session_ids\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(session_url) == len(abstract_url) == len(abstract_title) == len(session_no_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(session_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(workshop_abstract) == len(workshop_session_no) == len(workshop_session_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(workshop_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts = pd.DataFrame({'Session_URL': session_url,\n",
    "                            'Abstract_URL' : abstract_url,\n",
    "                            'Abstract_Title': abstract_title,\n",
    "                            'Session_No_v2': session_no_v2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts['Session_ID'] = df_abstracts['Session_URL'].apply(lambda x: re.sub(r'https\\S+((\\d){6})', r'\\1', x))\n",
    "df_abstracts['Abstract_URL'] = df_abstracts['Abstract_URL'].apply(lambda x: url + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workshops = pd.DataFrame({'Session_No': workshop_session_no,\n",
    "                            'Abstract_Text': workshop_abstract,\n",
    "                            'Session_URL': workshop_session_url})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workshops['Session_ID']= df_workshops['Session_URL'].apply(lambda x:re.sub(r'https\\S+((\\d){6})', r'\\1', x))\n",
    "df_workshops = df_workshops[df_workshops['Abstract_Text'].str.contains('Organizer\\(s\\)|Chair\\(s\\)') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workshops.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abstracts.drop_duplicates(subset = 'Abstract_URL', inplace = True)\n",
    "df_abstracts.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts.to_excel(r'{}\\2008Abstracts.xlsx'.format(dest))\n",
    "df_workshops.to_excel(r'{}\\2008Workshops.xlsx'.format(dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test extraction of abstract info here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_abstracts['Abstract_URL'].iloc[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(test)\n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.sub(r'\\r|\\n|\\t', '', soup.find_all('tr')[14].find('p').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract abstract details from abstract URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_num = []\n",
    "abs_keyword = []\n",
    "abs_type = []\n",
    "abs_text = []\n",
    "\n",
    "for link in df_abstracts['Abstract_URL']:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    # abstract number\n",
    "    try:\n",
    "        number = re.sub(r'\\S+abstractid=(\\d+)', r'\\1', link)\n",
    "        abs_num.append(number)\n",
    "    except:\n",
    "        abs_num.append(None)\n",
    "    \n",
    "    # abstract keywords\n",
    "    try:\n",
    "        keywords = [item.text for item in soup.find_all('a') if re.search('keyword', str(item.get('href')))]\n",
    "        keywords = ', '.join(keywords)\n",
    "        abs_keyword.append(keywords)\n",
    "    except:\n",
    "        abs_keyword.append(None)\n",
    "        \n",
    "    \n",
    "    # abstract type\n",
    "    try:\n",
    "        talk_type = re.sub(r'Type:|\\n|\\r|\\t', '', soup.find_all('tr')[5].text)\n",
    "        abs_type.append(talk_type)\n",
    "    except:\n",
    "        abs_type.append(None)\n",
    "    \n",
    "    # abstract text\n",
    "    try:\n",
    "        abstract_text = re.sub(r'\\r|\\n|\\t', '', soup.find_all('tr')[14].find('p').text)\n",
    "        abs_text.append(abstract_text)\n",
    "    except:\n",
    "        abs_text.append(None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts['Abstract_ID'] = abs_num\n",
    "df_abstracts['Abstract_Keywords'] = abs_keyword\n",
    "df_abstracts['Abstract_Type'] = abs_type\n",
    "df_abstracts['Abstract_Text'] = abs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts.to_excel(r'{}\\2008Abstracts.xlsx'.format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abstracts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_details = df_workshops.append(df_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_details.merge(df[['Session_Title', 'Sponsors', 'Type', 'Session_ID']], on = 'Session_ID', how= 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Unique'] = df_final['Session_ID'].map(str) + df_final['Abstract_ID'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop_duplicates(subset= 'Unique', inplace = True)\n",
    "df_final.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Session_No'] = df_final['Session_No'].apply(lambda x: x if bool(str(x).startswith('CE') or str(x)[0].isdigit()) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Session_No_Keep'] = df_final['Session_No_v2'].combine_first(df_final['Session_No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_final['Session_No']\n",
    "del df_final['Session_No_v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.rename(columns = {'Session_No_Keep': 'Session_No'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dest = r'C:\\Users\\Thaunga\\Scripts\\02. Presentations\\JSM\\Final Data'\n",
    "df_final.to_excel(r'{}\\2008_JSM_data.xlsx'.format(new_dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
